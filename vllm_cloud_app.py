#!/usr/bin/env python3
"""
vLLM —Å–µ—Ä–≤–µ—Ä –¥–ª—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
–£–±—Ä–∞–Ω—ã —Å–ª–æ–∂–Ω—ã–µ —Ç–∏–ø—ã –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
"""

import os
import sys
import logging
import time
from typing import List
from fastapi import FastAPI, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from pydantic import BaseModel
import uvicorn

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç transformers –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers.pipelines import pipeline
    import torch
    TRANSFORMERS_AVAILABLE = True
    logger.info("‚úÖ Transformers –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.error("‚ùå Transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# vLLM –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ü–∏—è
try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
    logger.info("‚úÖ vLLM –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    VLLM_AVAILABLE = False
    logger.warning("‚ö†Ô∏è vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

app = FastAPI(
    title="vLLM Cloud Server",
    version="1.0.0",
    description="–û–±–ª–∞—á–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
model = None
tokenizer = None
generator = None
model_info = {}
available_models = [
    "microsoft/DialoGPT-small",
    "microsoft/DialoGPT-medium", 
    "distilgpt2",
    "gpt2"
]

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 150
    temperature: float = 0.7
    top_p: float = 0.9
    stop: List[str] = []

class GenerateResponse(BaseModel):
    text: str
    prompt: str
    model: str
    generation_time: float

class ChangeModelRequest(BaseModel):
    model_name: str
    max_model_len: int = 512
    backend: str = "auto"

def get_optimal_model():
    """–í—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
    model_name = os.getenv("MODEL_NAME")
    if model_name:
        return model_name
    
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        if memory_gb >= 8:
            return "microsoft/DialoGPT-medium"
        elif memory_gb >= 4:
            return "distilgpt2"
        else:
            return "microsoft/DialoGPT-small"
    except:
        return "microsoft/DialoGPT-small"

def load_model_transformers(model_name=None, max_length=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ transformers"""
    global tokenizer, generator, model_info
    
    if model_name is None:
        model_name = get_optimal_model()
    if max_length is None:
        max_length = int(os.getenv("MAX_MODEL_LEN", "512"))
    
    logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ transformers: {model_name}")
    
    try:
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # –î–æ–±–∞–≤–ª—è–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generator = pipeline(
            "text-generation",
            model=model_name,
            tokenizer=tokenizer,
            device=-1,  # CPU
            torch_dtype=torch.float32,
            return_full_text=False,
            max_length=max_length
        )
        
        load_time = time.time() - start_time
        
        model_info = {
            "name": model_name,
            "max_length": max_length,
            "device": "cpu",
            "backend": "transformers",
            "load_time": load_time,
            "status": "loaded"
        }
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ transformers: {e}")
        model_info = {"status": "error", "error": str(e)}
        raise

def load_model_vllm(model_name=None, max_length=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ vLLM"""
    global model, model_info
    
    if not VLLM_AVAILABLE:
        raise ImportError("vLLM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    if model_name is None:
        model_name = get_optimal_model()
    if max_length is None:
        max_length = int(os.getenv("MAX_MODEL_LEN", "512"))
    
    logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ vLLM: {model_name}")
    
    try:
        start_time = time.time()
        
        model = LLM(
            model=model_name,
            max_model_len=max_length,
            enforce_eager=True,
            tensor_parallel_size=1,
            trust_remote_code=True
        )
        
        load_time = time.time() - start_time
        
        model_info = {
            "name": model_name,
            "max_length": max_length,
            "device": "cpu",
            "backend": "vllm",
            "load_time": load_time,
            "status": "loaded"
        }
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ vLLM: {e}")
        raise

def load_model(model_name=None, backend="auto"):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
    global model, tokenizer, generator, model_info
    
    # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –º–æ–¥–µ–ª—å
    model = None
    tokenizer = None
    generator = None
    
    if backend == "transformers" or (backend == "auto" and TRANSFORMERS_AVAILABLE):
        try:
            load_model_transformers(model_name)
            return
        except Exception as e:
            logger.warning(f"Transformers –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            if backend == "transformers":
                raise
    
    if backend == "vllm" or (backend == "auto" and VLLM_AVAILABLE):
        try:
            load_model_vllm(model_name)
            return
        except Exception as e:
            logger.warning(f"vLLM –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            if backend == "vllm":
                raise
    
    # Fallback –Ω–∞ transformers
    if TRANSFORMERS_AVAILABLE:
        load_model_transformers(model_name)
    else:
        raise RuntimeError("–ù–∏ vLLM, –Ω–∏ transformers –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")

@app.on_event("startup")
async def startup_event():
    """–°–æ–±—ã—Ç–∏–µ –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Cloud Server")
    
    try:
        load_model()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        global model_info
        model_info = {"status": "error", "error": str(e)}

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return {
        "message": "vLLM Cloud Server",
        "status": "running",
        "model": model_info.get("name", "not loaded"),
        "backend": model_info.get("backend", "unknown"),
        "backends_available": {
            "vllm": VLLM_AVAILABLE,
            "transformers": TRANSFORMERS_AVAILABLE
        },
        "endpoints": {
            "generate": "/generate",
            "change_model": "/change_model",
            "models": "/models",
            "health": "/health",
            "ui": "/ui"
        }
    }

@app.get("/health")
async def health():
    """Health check"""
    is_loaded = (model is not None) or (generator is not None)
    return {
        "status": "healthy" if is_loaded else "loading",
        "model_loaded": is_loaded,
        "model_info": model_info
    }

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if model is None and generator is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –µ—â–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    try:
        start_time = time.time()
        
        if model is not None and model_info.get("backend") == "vllm":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º vLLM
            stop_sequences = request.stop if request.stop else ["</s>", "<|endoftext|>"]
            sampling_params = SamplingParams(
                temperature=request.temperature,
                top_p=request.top_p,
                max_tokens=request.max_tokens,
                stop=stop_sequences
            )
            
            outputs = model.generate([request.prompt], sampling_params)
            generated_text = outputs[0].outputs[0].text
            
        elif generator is not None:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º transformers
            pad_token_id = tokenizer.eos_token_id if tokenizer else None
            result = generator(
                request.prompt,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True,
                pad_token_id=pad_token_id
            )
            
            if isinstance(result, list) and len(result) > 0:
                generated_text = result[0].get("generated_text", "").strip()
            else:
                generated_text = ""
        
        else:
            raise HTTPException(status_code=500, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        generation_time = time.time() - start_time
        
        return GenerateResponse(
            text=generated_text,
            prompt=request.prompt,
            model=model_info.get("name", "unknown"),
            generation_time=generation_time
        )
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def get_models():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    return {
        "current_model": model_info,
        "available_models": available_models,
        "available_backends": {
            "vllm": VLLM_AVAILABLE,
            "transformers": TRANSFORMERS_AVAILABLE
        }
    }

@app.post("/change_model")
async def change_model(request: ChangeModelRequest):
    """–°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏"""
    if request.model_name not in available_models:
        raise HTTPException(status_code=400, detail=f"–ú–æ–¥–µ–ª—å {request.model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
    
    try:
        logger.info(f"üîÑ –°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ {request.model_name} (backend: {request.backend})")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏
        global model_info
        model_info = {"status": "loading", "name": request.model_name}
        
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        load_model(request.model_name, request.backend)
        
        return {
            "message": f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ {request.model_name}",
            "model_info": model_info
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏: {e}")
        model_info = {"status": "error", "error": str(e)}
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")

@app.get("/stats")
async def get_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        import psutil
        
        return {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "model_info": model_info
        }
    except ImportError:
        return {"error": "psutil not available"}

# –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –∏ UI
app.mount("/static", StaticFiles(directory="/app/static"), name="static")

@app.get("/ui", response_class=HTMLResponse)
async def get_ui():
    """–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    return FileResponse("/app/static/index.html")

if __name__ == "__main__":
    port = int(os.getenv("PORT", "8080"))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"üåê –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ {host}:{port}")
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True
    )