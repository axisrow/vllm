# Dockerfile –¥–ª—è vLLM –Ω–∞ Sliplane (–æ–±–ª–∞—á–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ)
FROM nvidia/cuda:12.1-devel-ubuntu22.04

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    curl \
    wget \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# –°–æ–∑–¥–∞–µ–º —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
WORKDIR /app

# –û–±–Ω–æ–≤–ª—è–µ–º pip
RUN pip3 install --upgrade pip

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch (CPU –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–±–ª–∞—á–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏)
RUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM
RUN pip3 install vllm

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
RUN pip3 install \
    transformers \
    accelerate \
    fastapi \
    uvicorn \
    requests \
    numpy \
    aiofiles

# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ vLLM –¥–ª—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
RUN cat > /app/vllm_cloud_app.py << 'EOF'
#!/usr/bin/env python3
"""
vLLM —Å–µ—Ä–≤–µ—Ä –¥–ª—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ Sliplane
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–Ω–æ–π —Å—Ä–µ–¥–µ
"""

import os
import sys
import logging
import asyncio
from typing import List, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from pydantic import BaseModel
import uvicorn
from vllm import LLM, SamplingParams
import torch
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="vLLM Cloud Server",
    version="1.0.0",
    description="–û–±–ª–∞—á–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º vLLM"
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –º–æ–¥–µ–ª–∏
llm_model = None
model_info = {}

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 150
    temperature: float = 0.7
    top_p: float = 0.9
    stop: Optional[List[str]] = None

class GenerateResponse(BaseModel):
    text: str
    prompt: str
    model: str
    generation_time: float

class BatchGenerateRequest(BaseModel):
    prompts: List[str]
    max_tokens: int = 150
    temperature: float = 0.7
    top_p: float = 0.9
    stop: Optional[List[str]] = None

def get_optimal_model():
    """–í—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
    model_name = os.getenv("MODEL_NAME")
    if model_name:
        return model_name
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞–º—è—Ç–∏
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        if memory_gb >= 8:
            return "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        elif memory_gb >= 4:
            return "microsoft/DialoGPT-medium"
        else:
            return "microsoft/DialoGPT-small"
    except:
        return "microsoft/DialoGPT-small"

def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ vLLM —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –æ–±–ª–∞–∫–∞"""
    global llm_model, model_info
    
    model_name = get_optimal_model()
    max_model_len = int(os.getenv("MAX_MODEL_LEN", "512"))
    
    logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_name}")
    logger.info(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {max_model_len}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
    if torch.cuda.is_available():
        logger.info(f"üöÄ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}")
        device_config = {
            "tensor_parallel_size": 1,
            "gpu_memory_utilization": float(os.getenv("GPU_MEMORY_UTILIZATION", "0.8")),
        }
    else:
        logger.info("üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU —Ä–µ–∂–∏–º")
        device_config = {
            "tensor_parallel_size": 1,
            "gpu_memory_utilization": 0.0,
        }
    
    try:
        start_time = time.time()
        
        llm_model = LLM(
            model=model_name,
            max_model_len=max_model_len,
            enforce_eager=True,  # –£–ª—É—á—à–∞–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
            trust_remote_code=True,  # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π
            **device_config
        )
        
        load_time = time.time() - start_time
        
        model_info = {
            "name": model_name,
            "max_length": max_model_len,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "load_time": load_time,
            "status": "loaded"
        }
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        model_info = {"status": "error", "error": str(e)}
        raise

@app.on_event("startup")
async def startup_event():
    """–°–æ–±—ã—Ç–∏–µ –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ vLLM Cloud Server")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å startup
    try:
        load_model()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return {
        "message": "vLLM Cloud Server",
        "status": "running",
        "model": model_info.get("name", "not loaded"),
        "endpoints": {
            "generate": "/generate",
            "batch": "/batch",
            "models": "/models",
            "health": "/health",
            "ui": "/ui"
        }
    }

@app.get("/health")
async def health():
    """Health check –¥–ª—è Sliplane"""
    return {
        "status": "healthy" if llm_model is not None else "loading",
        "model_loaded": llm_model is not None,
        "model_info": model_info
    }

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if llm_model is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –µ—â–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è")
    
    try:
        start_time = time.time()
        
        sampling_params = SamplingParams(
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            stop=request.stop or ["</s>", "<|endoftext|>", "\n\n"]
        )
        
        outputs = llm_model.generate([request.prompt], sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        generation_time = time.time() - start_time
        
        return GenerateResponse(
            text=generated_text.strip(),
            prompt=request.prompt,
            model=model_info.get("name", "unknown"),
            generation_time=generation_time
        )
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch")
async def batch_generate(request: BatchGenerateRequest):
    """–ë–∞—Ç—á–µ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤"""
    if llm_model is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –µ—â–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è")
    
    try:
        start_time = time.time()
        
        sampling_params = SamplingParams(
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            stop=request.stop or ["</s>", "<|endoftext|>", "\n\n"]
        )
        
        outputs = llm_model.generate(request.prompts, sampling_params)
        
        results = []
        for output in outputs:
            results.append({
                "prompt": output.prompt,
                "text": output.outputs[0].text.strip(),
                "model": model_info.get("name", "unknown")
            })
        
        generation_time = time.time() - start_time
        
        return {
            "results": results,
            "total_prompts": len(request.prompts),
            "generation_time": generation_time
        }
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –±–∞—Ç—á–µ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def get_models():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    return {
        "current_model": model_info,
        "available_models": [
            "microsoft/DialoGPT-small",
            "microsoft/DialoGPT-medium", 
            "distilgpt2",
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        ]
    }

@app.get("/stats")
async def get_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        import psutil
        
        return {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "model_info": model_info
        }
    except ImportError:
        return {"error": "psutil not available"}

if __name__ == "__main__":
    port = int(os.getenv("PORT", "8080"))  # Sliplane –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 8080
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"üåê –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ {host}:{port}")
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True
    )
EOF

# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
RUN mkdir -p /app/static && cat > /app/static/index.html << 'EOF'
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>vLLM Cloud Server</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            border-radius: 10px;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
            margin-bottom: 30px;
        }
        textarea {
            width: 100%;
            min-height: 120px;
            padding: 15px;
            border: 2px solid #ddd;
            border-radius: 8px;
            font-size: 16px;
            resize: vertical;
            box-sizing: border-box;
        }
        .controls {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr auto;
            gap: 15px;
            margin: 20px 0;
            align-items: end;
        }
        label {
            font-weight: 600;
            color: #555;
        }
        input[type="number"], input[type="range"] {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        button {
            padding: 12px 24px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            transition: transform 0.2s;
        }
        button:hover {
            transform: translateY(-2px);
        }
        button:disabled {
            background: #ccc;
            transform: none;
            cursor: not-allowed;
        }
        .result {
            margin-top: 20px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }
        .loading {
            color: #666;
            font-style: italic;
        }
        .error {
            background: #ffe6e6;
            border-left-color: #dc3545;
            color: #721c24;
        }
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }
        .stat-card {
            background: #e9ecef;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }
        .stat-value {
            font-size: 24px;
            font-weight: bold;
            color: #495057;
        }
        .stat-label {
            font-size: 14px;
            color: #6c757d;
            margin-top: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ vLLM Cloud Server</h1>
        
        <textarea id="prompt" placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞...">–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –æ –∫–æ—Å–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏—è—Ö.</textarea>
        
        <div class="controls">
            <div>
                <label for="maxTokens">–ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤:</label>
                <input type="number" id="maxTokens" value="150" min="10" max="500">
            </div>
            <div>
                <label for="temperature">–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞:</label>
                <input type="range" id="temperature" value="0.7" min="0.1" max="2.0" step="0.1">
                <span id="tempValue">0.7</span>
            </div>
            <div>
                <label for="topP">Top-p:</label>
                <input type="range" id="topP" value="0.9" min="0.1" max="1.0" step="0.1">
                <span id="topPValue">0.9</span>
            </div>
            <button onclick="generate()" id="generateBtn">–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å</button>
        </div>
        
        <div id="result"></div>
        
        <div class="stats" id="stats"></div>
    </div>

    <script>
        // –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π —Å–ª–∞–π–¥–µ—Ä–æ–≤
        document.getElementById('temperature').oninput = function() {
            document.getElementById('tempValue').textContent = this.value;
        }
        
        document.getElementById('topP').oninput = function() {
            document.getElementById('topPValue').textContent = this.value;
        }
        
        async function generate() {
            const prompt = document.getElementById('prompt').value.trim();
            const maxTokens = parseInt(document.getElementById('maxTokens').value);
            const temperature = parseFloat(document.getElementById('temperature').value);
            const topP = parseFloat(document.getElementById('topP').value);
            
            const resultDiv = document.getElementById('result');
            const generateBtn = document.getElementById('generateBtn');
            
            if (!prompt) {
                alert('–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å');
                return;
            }
            
            generateBtn.disabled = true;
            generateBtn.textContent = '–ì–µ–Ω–µ—Ä–∏—Ä—É—é...';
            resultDiv.innerHTML = '<div class="loading">‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...</div>';
            
            try {
                const response = await fetch('/generate', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({
                        prompt: prompt,
                        max_tokens: maxTokens,
                        temperature: temperature,
                        top_p: topP
                    })
                });
                
                const data = await response.json();
                
                if (response.ok) {
                    resultDiv.innerHTML = `
                        <div class="result">
                            <h3>üìù –†–µ–∑—É–ª—å—Ç–∞—Ç:</h3>
                            <p><strong>${data.text}</strong></p>
                            <small>–ú–æ–¥–µ–ª—å: ${data.model} | –í—Ä–µ–º—è: ${data.generation_time.toFixed(2)}s</small>
                        </div>
                    `;
                } else {
                    throw new Error(data.detail || '–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏');
                }
            } catch (error) {
                resultDiv.innerHTML = `
                    <div class="result error">
                        <h3>‚ùå –û—à–∏–±–∫–∞:</h3>
                        <p>${error.message}</p>
                    </div>
                `;
            } finally {
                generateBtn.disabled = false;
                generateBtn.textContent = '–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å';
            }
        }
        
        // –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
        async function loadStats() {
            try {
                const [modelsRes, statsRes] = await Promise.all([
                    fetch('/models'),
                    fetch('/stats')
                ]);
                
                const models = await modelsRes.json();
                const stats = await statsRes.json();
                
                const statsDiv = document.getElementById('stats');
                statsDiv.innerHTML = `
                    <div class="stat-card">
                        <div class="stat-value">${models.current_model.name || 'N/A'}</div>
                        <div class="stat-label">–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">${models.current_model.device || 'N/A'}</div>
                        <div class="stat-label">–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">${stats.memory_percent ? stats.memory_percent.toFixed(1) + '%' : 'N/A'}</div>
                        <div class="stat-label">–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-value">${models.current_model.status || 'unknown'}</div>
                        <div class="stat-label">–°—Ç–∞—Ç—É—Å</div>
                    </div>
                `;
            } catch (error) {
                console.error('–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:', error);
            }
        }
        
        // –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        loadStats();
        
        // –û–±—Ä–∞–±–æ—Ç–∫–∞ Enter –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        document.getElementById('prompt').addEventListener('keydown', function(e) {
            if (e.ctrlKey && e.key === 'Enter') {
                generate();
            }
        });
    </script>
</body>
</html>
EOF

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é
RUN cat >> /app/vllm_cloud_app.py << 'EOF'

# –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –∏ UI
app.mount("/static", StaticFiles(directory="/app/static"), name="static")

@app.get("/ui", response_class=HTMLResponse)
async def get_ui():
    """–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    return FileResponse("/app/static/index.html")
EOF

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º psutil –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
RUN pip3 install psutil

# –°–æ–∑–¥–∞–µ–º requirements.txt –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
RUN cat > /app/requirements.txt << 'EOF'
vllm>=0.2.0
torch>=2.0.0
transformers>=4.30.0
fastapi>=0.100.0
uvicorn[standard]>=0.20.0
accelerate>=0.20.0
requests>=2.28.0
numpy>=1.24.0
aiofiles>=23.0.0
psutil>=5.9.0
EOF

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
ENV MODEL_NAME=microsoft/DialoGPT-small
ENV MAX_MODEL_LEN=512
ENV GPU_MEMORY_UTILIZATION=0.8
ENV PORT=8080
ENV HOST=0.0.0.0

# –û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ä—Ç –¥–ª—è Sliplane
EXPOSE 8080

# Health check –¥–ª—è –æ–±–ª–∞—á–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8080/health || exit 1

# –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞
CMD ["python3", "/app/vllm_cloud_app.py"]